<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>技术/AI/笔记 on LX 知识库</title><link>https://namejlt.github.io/categories/%E6%8A%80%E6%9C%AF/ai/%E7%AC%94%E8%AE%B0/</link><description>Recent content in 技术/AI/笔记 on LX 知识库</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Thu, 22 May 2025 18:00:00 +0800</lastBuildDate><atom:link href="https://namejlt.github.io/categories/%E6%8A%80%E6%9C%AF/ai/%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml"/><item><title>AI 笔记-本地大模型部署</title><link>https://namejlt.github.io/posts/tech/ai/note/local-llm/</link><pubDate>Thu, 22 May 2025 18:00:00 +0800</pubDate><guid>https://namejlt.github.io/posts/tech/ai/note/local-llm/</guid><description>
&lt;h2 id="概述">概述&lt;/h2>
&lt;p>本文主要简述本地部署大模型以及使用。&lt;/p>
&lt;p>本地部署大模型不仅可以保护数据隐私，还能降低API调用成本，减少网络延迟，并且在离线环境中使用。本文将详细介绍多种本地部署大模型的方法、步骤、验证过程以及适用场景，帮助读者成功在本地环境中运行自己的大语言模型。&lt;/p>
&lt;h2 id="本地部署大模型的优势">本地部署大模型的优势&lt;/h2>
&lt;ol>
&lt;li>&lt;strong>数据隐私与安全&lt;/strong>：敏感数据不需要发送到第三方服务器&lt;/li>
&lt;li>&lt;strong>降低成本&lt;/strong>：避免按token计费的API调用费用&lt;/li>
&lt;li>&lt;strong>减少延迟&lt;/strong>：无需网络传输，响应更快&lt;/li>
&lt;li>&lt;strong>离线使用&lt;/strong>：不依赖互联网连接&lt;/li>
&lt;li>&lt;strong>自定义与控制&lt;/strong>：可以根据需求调整和优化模型（非必要不微调）&lt;/li>
&lt;/ol>
&lt;h2 id="硬件要求">硬件要求&lt;/h2>
&lt;p>在开始部署前，需要了解不同规模模型的硬件需求：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>模型规模&lt;/th>
&lt;th>参数量&lt;/th>
&lt;th>最低内存要求&lt;/th>
&lt;th>推荐GPU&lt;/th>
&lt;th>CPU可行性&lt;/th>
&lt;th>应用场景&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>小型模型&lt;/td>
&lt;td>1-3B&lt;/td>
&lt;td>8GB RAM&lt;/td>
&lt;td>4GB VRAM&lt;/td>
&lt;td>可行但较慢&lt;/td>
&lt;td>个人demo&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>中型模型&lt;/td>
&lt;td>7-13B&lt;/td>
&lt;td>16GB RAM&lt;/td>
&lt;td>8-16GB VRAM&lt;/td>
&lt;td>勉强可行&lt;/td>
&lt;td>个人使用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>大型模型&lt;/td>
&lt;td>30-70B&lt;/td>
&lt;td>32GB+ RAM&lt;/td>
&lt;td>24GB+ VRAM&lt;/td>
&lt;td>不推荐&lt;/td>
&lt;td>企业应用&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="多种部署方式对比">多种部署方式对比&lt;/h2>
&lt;h3 id="1-独立应用程序">1. 独立应用程序&lt;/h3>
&lt;p>&lt;strong>代表工具&lt;/strong>：LM Studio, Ollama, LocalAI&lt;/p>
&lt;p>&lt;strong>优点&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>用户友好，安装简单&lt;/li>
&lt;li>图形界面操作&lt;/li>
&lt;li>预配置多种模型&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>缺点&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>自定义能力有限&lt;/li>
&lt;li>集成到其他应用可能受限&lt;/li>
&lt;/ul>
&lt;h3 id="2-python框架">2. Python框架&lt;/h3>
&lt;p>&lt;strong>代表工具&lt;/strong>：LangChain, llama.cpp, Hugging Face Transformers&lt;/p>
&lt;p>&lt;strong>优点&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>高度灵活和可定制&lt;/li>
&lt;li>可与现有Python项目集成&lt;/li>
&lt;li>支持更多高级功能&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>缺点&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>需要编程知识&lt;/li>
&lt;li>配置过程较复杂&lt;/li>
&lt;/ul>
&lt;h3 id="3-docker容器">3. Docker容器&lt;/h3>
&lt;p>&lt;strong>代表工具&lt;/strong>：text-generation-webui, LocalAI Docker&lt;/p></description></item></channel></rss>