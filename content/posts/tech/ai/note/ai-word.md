---
title: "AI 笔记-大模型常见名词以及关系"
date: 2025-06-26T17:50:00+08:00
draft: false
toc: true
categories: ["技术/AI/笔记"]
tags: ["AI", "大模型"]
---

## 名词解释

我们可以将这些名词想象成一个金字塔结构，从顶层的应用概念到底层的核心技术，逐一解析。

### 金字塔顶层：概念与应用 (Concepts & Applications)

---

#### 1. **生成式AI (Generative AI)**

* **名词解释**：
    生成式AI是一类人工智能的总称，它的核心能力是**创造和生成全新的、原创的内容**，而不是仅仅做分析、分类或识别。这些内容可以是文本、图片、音频、代码、视频甚至是三维模型。
* **例子**：
    * 输入“一只穿着宇航服的猫在月球上”，生成一张对应的图片（如Midjourney, Stable Diffusion）。
    * 输入“帮我写一首关于夏天的诗”，生成一首诗（如GPT系列）。
    * 输入一段旋律，生成完整的乐曲。

#### 2. **大语言模型 (Large Language Model, LLM)**

* **名词解释**：
    大语言模型是生成式AI在**自然语言处理（NLP）**领域最重要、最成功的应用。它特指那些在海量文本数据上进行训练、拥有巨量参数（通常在数十亿到数万亿之间）并能够理解和生成人类语言的模型。LLM的核心任务是**预测文本序列中下一个词的概率**。
* **与生成式AI的关系**：
    **LLM是生成式AI的一个子集和典型代表**。可以说，生成式AI是“属”，LLM是“种”。所有LLM都是生成式AI，但生成式AI还包括图像生成模型、音频生成模型等。

### 金字塔中层：模型架构与训练方法 (Architecture & Training)

这一层解释了LLM是如何构建和训练出来的。

---

#### 3. **Transformer架构 (Transformer Architecture)**

* **名词解释**：
    Transformer是2017年由Google在论文《Attention Is All You Need》中提出的一种深度学习模型架构。它的出现彻底改变了NLP领域，并成为当今几乎所有主流LLM（如GPT系列、BERT、LLaMA）的**基础和核心骨架**。它摒弃了传统的循环神经网络（RNN）和卷积神经网络（CNN），完全依赖于“注意力机制”来处理序列数据。
* **核心优势**：
    * **并行计算能力强**：相比RNN必须按顺序处理文本，Transformer可以同时处理整个输入序列，极大地提高了训练效率。
    * **长距离依赖捕捉**：通过注意力机制，它能更好地理解句子中相隔较远的词语之间的关系。

#### 4. **编码器-解码器 (Encoder-Decoder) 架构**

* **名词解释**：
    这是在Transformer之前就广泛应用于机器翻译等任务的一种通用框架。
    * **编码器 (Encoder)**：负责读取和“理解”输入序列（比如一句中文），将其压缩成一个包含语义信息的中间状态（通常是一个向量）。
    * **解码器 (Decoder)**：接收编码器的中间状态，并逐词生成输出序列（比如翻译后的英文）。
* **与Transformer的关系**：
    最初的Transformer模型就是一个标准的Encoder-Decoder架构，非常适合“输入一个序列，输出另一个序列”的任务（如翻译）。后来，这个架构被灵活拆分：
    * **仅编码器模型 (Encoder-only)**：如BERT。它擅长**理解**文本，但不擅长生成。非常适合做文本分类、情感分析、命名实体识别等任务。
    * **仅解码器模型 (Decoder-only)**：如GPT系列。它接收一段文本（Prompt），然后不断地向后生成新的文本。这是目前**主流聊天型、生成型LLM**采用的架构。
    * **完整的编码器-解码器模型**：如T5、BART。它们在需要对输入进行深度理解后再进行生成的任务上表现出色。

#### 5. **注意力机制 (Attention Mechanism)**

* **名词解释**：
    这是Transformer架构的**灵魂和精髓**。它模仿了人类的注意力，让模型在处理一个词时，能够给予输入序列中其他所有词不同的“关注度”或“权重”。对于当前词，哪些词对它的意义最重要，注意力权重就更高。
* **例子**：
    在句子“The animal didn't cross the street because **it** was too tired.”中，注意力机制能帮助模型在处理单词"it"时，将绝大部分注意力放在"animal"上，而不是"street"，从而正确理解"it"指代的是"animal"。
* **与Transformer的关系**：
    **Transformer就是完全基于注意力机制构建的**。其核心组件“多头自注意力（Multi-Head Self-Attention）”允许模型从不同角度（多个“头”）同时关注输入序列的不同部分，捕捉丰富的句法和语义关系。

#### 6. **预训练 (Pre-training)**

* **名词解释**：
    这是训练大模型的**第一步，也是最耗费资源的一步**。在这个阶段，模型会在海量的、未经标注的通用文本数据（例如整个互联网的公开文本、书籍、维基百科）上进行学习。学习的目标非常简单，通常是“掩码语言模型”（像BERT，随机遮住一些词让模型去猜）或“下一个词预测”（像GPT，根据前面的词预测下一个词）。
* **目的**：
    通过这个过程，模型能够学习到通用的语言知识，包括词汇、语法、句法结构，甚至是世界知识和一定的推理能力。预训练完成后，模型就成了一个知识渊博但“不通人情世故”的“通才”。

#### 7. **微调 (Fine-tuning)**

* **名词解释**：
    预训练完成后，为了让模型适应特定任务或特定领域的风格，需要进行微调。这个阶段会使用一个**规模小得多但质量更高的、有标注的**数据集来继续训练模型。
* **例子**：
    * **指令微调 (Instruction Tuning)**：使用大量的“指令-回答”数据对，让模型学会听从人类的指令。这是让模型从只会“续写”变成会“对话”的关键步骤。
    * **领域微调**：使用法律、医疗或金融领域的专业文本，让通用模型变成一个更懂行的“领域专家”。
* **与预训练的关系**：
    **先预训练，后微调**。预训练赋予模型广泛的通用知识，微调则是在此基础上，对模型进行“精雕细琢”，使其在特定能力上“专精”或“对齐”人类的价值观和交流方式。

### 金字塔底层：交互与内部表征 (Interaction & Representation)

---

#### 8. **提示工程 (Prompt Engineering)**

* **名词解释**：
    这不是训练模型，而是**如何与已经训练好的模型进行高效沟通的艺术和科学**。通过精心设计输入给模型的问题或指令（即Prompt），可以更好地引导模型输出我们想要的结果。
* **例子**：
    * 简单提问：“解释一下黑洞。”
    * 更好的Prompt：“以一个五年级学生能听懂的方式，用一个生动的比喻来解释什么是黑洞。”
* **与微调的关系**：
    * **微调改变的是模型的“内在参数”**，它是一个成本较高的训练过程。
    * **提示工程改变的是模型的“输入”**，它是一种低成本、灵活的与模型交互的技巧。对于大多数用户来说，他们主要通过提示工程来使用LLM。

#### 9. **词嵌入 (Embeddings)**

* **名词解释**：
    计算机无法直接理解文字，必须将其转换成数字。词嵌入是一种将词语或文本片段**表示为高维空间中密集向量**的技术。在这个向量空间中，意思相近的词语，其向量距离也更近。
* **例子**：
    “国王”和“女王”的向量在空间中的位置会很接近。有趣的是，`vector('国王') - vector('男人') + vector('女人')` 的计算结果会非常接近 `vector('女王')` 的向量。
* **在LLM中的作用**：
    词嵌入是LLM处理文本的**第一步**。输入的文本首先通过一个嵌入层（Embedding Layer）转换成向量，然后这些向量才会被送入Transformer的主体结构中进行处理。

#### 10. **幻觉 (Hallucination)**

* **名词解释**：
    指大模型在生成内容时，**编造出一些看似合理但实际上是错误的、不存在的或无中生有的信息**。这是当前LLM最主要的缺陷之一。
* **产生原因**：
    * 模型本质上是基于概率的“下一个词预测机”，它总会尽力生成最连贯的句子，哪怕这需要它编造事实。
    * 训练数据中可能包含错误或过时的信息。
    * 模型的推理能力有限，在复杂逻辑链条中容易出错。
* **与模型的关系**：
    幻觉是LLM工作原理的**固有副产品**。理解并缓解幻觉是使用和发展LLM的一个关键挑战。

## 关系总结

**一个典型的现代大语言模型（LLM）是这样运作的：**

1.  它首先是一个**生成式AI**的实例，专注于语言领域。
2.  它的底层架构是**Transformer**，而这个架构的核心是**注意力机制**。根据任务的不同，它可能采用**编码器-解码器**架构的全部或部分（如仅解码器）。
3.  它的诞生经历了两个阶段：首先是漫长且昂贵的**预训练**，让它学习海量的通用知识；然后是更具针对性的**微调**，让它学会对话或掌握特定领域的知识。
4.  当我们使用它时，我们输入的文字（**Prompt**）首先被转换成**词嵌入**向量。
5.  模型内部的Transformer结构通过注意力机制处理这些向量，并逐词生成回答。我们通过**提示工程**来优化我们的输入，以获得更好的输出。
6.  在整个过程中，我们需要警惕模型可能会产生**幻觉**，输出不准确的信息。